"""
æ§‹é€ ä»•æ§˜ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  - ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³
"""

from __future__ import annotations

import importlib
import json
import sys
from pathlib import Path
from typing import Any, Generic, TypeVar

import jsonschema
import networkx as nx
import yaml
from pydantic import BaseModel, Field, field_validator


# ==================== åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒžãƒ¼ã‚«ãƒ¼ ====================

T = TypeVar("T")


class Check(Generic[T]):
    """åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®Checkå‚ç…§ãƒžãƒ¼ã‚«ãƒ¼

    Usage:
        Annotated[Out, Check["module.path.check_function"]]
    """

    def __class_getitem__(cls, item: str) -> type:
        """æ–‡å­—åˆ—ã§ãƒã‚§ãƒƒã‚¯é–¢æ•°ã‚’å‚ç…§"""
        return type(f"Check[{item}]", (), {"__check_ref__": item})


class ExampleValue(Generic[T]):
    """åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®Exampleå€¤ãƒžãƒ¼ã‚«ãƒ¼

    Usage:
        Annotated[In, ExampleValue[{"text": "hello world"}]]
    """

    def __class_getitem__(cls, item: dict[str, Any]) -> type:
        """è¾žæ›¸ã§ä¾‹ç¤ºå€¤ã‚’å‚ç…§"""
        return type(f"ExampleValue[{item}]", (), {"__example_value__": item})


# ==================== ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾© ====================


class CheckDef(BaseModel):
    """ãƒã‚§ãƒƒã‚¯é–¢æ•°å®šç¾©"""

    id: str
    description: str
    impl: str  # "module:function"å½¢å¼
    file_path: str


class Example(BaseModel):
    """æ¤œè¨¼ç”¨å…¥åŠ›ãƒ»æœŸå¾…å€¤å®šç¾©"""

    id: str
    description: str
    input: dict[str, Any]
    expected: dict[str, Any]


class DataType(BaseModel):
    """ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾©"""

    model_config = {"protected_namespaces": ()}

    id: str
    description: str
    check_ids: list[str] = Field(default_factory=list)
    example_ids: list[str] = Field(default_factory=list)
    schema_def: dict[str, Any] = Field(alias="schema")  # JSON Schema


class Parameter(BaseModel):
    """é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©"""

    name: str
    datatype_ref: str | None = None
    native: str | None = None  # "builtins:type"å½¢å¼

    @field_validator("datatype_ref", "native")
    @classmethod
    def at_least_one(cls, v: str | None, info) -> str | None:
        values = info.data
        if "datatype_ref" in values and "native" in values:
            if values.get("datatype_ref") is None and values.get("native") is None:
                raise ValueError("datatype_ref ã¾ãŸã¯ native ã®ã„ãšã‚Œã‹ãŒå¿…è¦ã§ã™")
        return v


class Transform(BaseModel):
    """å‡¦ç†é–¢æ•°å®šç¾©"""

    id: str
    description: str
    impl: str  # "module:function"å½¢å¼
    file_path: str
    parameters: list[Parameter]
    return_datatype_ref: str | None = None
    return_native: str | None = None  # "module:type"å½¢å¼ï¼ˆæˆ»ã‚Šå€¤ã®åž‹æŒ‡å®šï¼‰
    default_args: dict[str, Any] = Field(default_factory=dict)


class DAGEdge(BaseModel):
    """DAGã‚¨ãƒƒã‚¸å®šç¾©"""

    model_config = {"populate_by_name": True}

    from_: str = Field(alias="from")
    to: str | None


class Meta(BaseModel):
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""

    name: str
    description: str


class DAGStage(BaseModel):
    """DAG stage definition (for unified dag_stages representation)"""

    stage_id: str
    description: str
    selection_mode: str = "single"  # single, exclusive, multiple
    max_select: int | None = None
    input_type: str
    output_type: str
    candidates: list[str] = Field(default_factory=list)  # transform_ids
    default_transform_id: str | None = None


class Spec(BaseModel):
    """ä»•æ§˜å…¨ä½“ã®ãƒ«ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«"""

    version: str
    meta: Meta
    checks: list[CheckDef] = Field(default_factory=list)
    examples: list[Example] = Field(default_factory=list)
    datatypes: list[DataType] = Field(default_factory=list)
    transforms: list[Transform] = Field(default_factory=list)
    dag: list[DAGEdge] = Field(default_factory=list)
    dag_stages: list[DAGStage] = Field(default_factory=list)


# ==================== ä»•æ§˜èª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼ ====================


def load_spec(spec_path: str | Path) -> Spec:
    """YAML/JSONä»•æ§˜ã‚’èª­ã¿è¾¼ã¿ã€Pydanticã§æ¤œè¨¼"""
    spec_path = Path(spec_path)
    with open(spec_path) as f:
        if spec_path.suffix in [".yaml", ".yml"]:
            data = yaml.safe_load(f)
        elif spec_path.suffix == ".json":
            data = json.load(f)
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {spec_path.suffix}")

    spec = Spec(**data)
    _convert_dag_to_stages(spec)
    return spec


def _convert_dag_to_stages(spec: Spec) -> None:
    """Convert legacy dag format to dag_stages format

    If dag_stages is empty but dag exists, generate dag_stages from DAG topology.
    Each transform becomes a single-mode stage with one candidate.
    """
    if spec.dag_stages:
        # Already has dag_stages, no conversion needed
        return

    if not spec.dag:
        # No dag to convert
        return

    # Build transform lookup
    transform_by_id = {t.id: t for t in spec.transforms}

    # Build DAG graph to determine execution order
    import networkx as nx

    G = nx.DiGraph()
    for edge in spec.dag:
        if edge.to is not None:
            G.add_edge(edge.from_, edge.to)
        else:
            # Terminal node (no outgoing edges)
            G.add_node(edge.from_)

    # Topological sort to get execution order
    try:
        ordered_transforms = list(nx.topological_sort(G))
    except nx.NetworkXError:
        # Graph has cycles or other issues, fallback to original order
        ordered_transforms = [edge.from_ for edge in spec.dag]

    # Create single-mode stages for each transform
    stages = []
    for i, transform_id in enumerate(ordered_transforms):
        if transform_id not in transform_by_id:
            continue

        transform = transform_by_id[transform_id]

        # Determine input/output types
        input_type = ""
        output_type = ""

        if transform.parameters:
            first_param = transform.parameters[0]
            input_type = first_param.datatype_ref or first_param.native or "Any"

        output_type = transform.return_datatype_ref or transform.return_native or "Any"

        stage = DAGStage(
            stage_id=f"stage_{i+1}_{transform_id}",
            description=transform.description or f"Stage {i+1}: {transform_id}",
            selection_mode="single",
            input_type=input_type,
            output_type=output_type,
            candidates=[transform_id],
            default_transform_id=transform_id,
        )
        stages.append(stage)

    spec.dag_stages = stages


# ==================== ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ ====================


def _build_type_annotation(
    spec: Spec, param: Parameter, app_root: Path
) -> tuple[str, set[str]]:
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ï¼ˆInputãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ï¼šExampleã®ã¿é©ç”¨ï¼‰

    Returns:
        (åž‹æ–‡å­—åˆ—, importã‚»ãƒƒãƒˆ)
    """
    imports = set()

    # ãƒ™ãƒ¼ã‚¹åž‹ã‚’æ±ºå®š
    base_type = "dict"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    if param.native:
        # native ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã†
        module, type_name = param.native.split(":")
        if module != "builtins":
            # pandas ã®å ´åˆã¯ pd ã¨ã—ã¦ import
            if module == "pandas":
                imports.add("import pandas as pd")
                base_type = f"pd.{type_name}"
            else:
                imports.add(f"import {module}")
                base_type = f"{module}.{type_name}"
        else:
            base_type = type_name

    # Example ã¯ datatype_ref ã‹ã‚‰å–å¾—ï¼ˆInputãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ï¼‰
    example_annotations = []

    if param.datatype_ref:
        datatype = next(
            (dt for dt in spec.datatypes if dt.id == param.datatype_ref), None
        )
        if datatype:
            # Exampleå‚ç…§ã®ã¿è¿½åŠ ï¼ˆCheckã¯è¿½åŠ ã—ãªã„ï¼‰
            for example_id in datatype.example_ids:
                example = next((e for e in spec.examples if e.id == example_id), None)
                if example:
                    example_annotations.append(f"ExampleValue[{example.input}]")
                    imports.add("from spec2code.engine import ExampleValue")

    # Annotatedã‚’ä½¿ç”¨
    if example_annotations:
        imports.add("from typing import Annotated")
        annotations = ", ".join(example_annotations)
        return f"Annotated[{base_type}, {annotations}]", imports

    return base_type, imports


def _build_return_annotation(
    spec: Spec, transform: Transform, app_root: Path
) -> tuple[str, set[str]]:
    """æˆ»ã‚Šå€¤ã®åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ï¼ˆOutputãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ï¼šCheckã®ã¿é©ç”¨ï¼‰

    Returns:
        (åž‹æ–‡å­—åˆ—, importã‚»ãƒƒãƒˆ)
    """
    imports = set()

    # ãƒ™ãƒ¼ã‚¹åž‹ã‚’æ±ºå®š
    base_type = "dict"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    if transform.return_native:
        # return_native ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã†
        module, type_name = transform.return_native.split(":")
        if module != "builtins":
            # pandas ã®å ´åˆã¯ pd ã¨ã—ã¦ import
            if module == "pandas":
                imports.add("import pandas as pd")
                base_type = f"pd.{type_name}"
            else:
                imports.add(f"import {module}")
                base_type = f"{module}.{type_name}"
        else:
            base_type = type_name

    # Check ã¯ return_datatype_ref ã‹ã‚‰å–å¾—ï¼ˆOutputãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ï¼‰
    check_annotations = []

    if transform.return_datatype_ref:
        datatype = next(
            (dt for dt in spec.datatypes if dt.id == transform.return_datatype_ref),
            None,
        )
        if datatype:
            # Checkå‚ç…§ã®ã¿è¿½åŠ ï¼ˆExampleã¯è¿½åŠ ã—ãªã„ï¼‰
            for check_id in datatype.check_ids:
                check_def = next((c for c in spec.checks if c.id == check_id), None)
                if check_def:
                    check_annotations.append(f'Check["{check_def.impl}"]')
                    imports.add("from spec2code.engine import Check")

    # Annotatedã‚’ä½¿ç”¨
    if check_annotations:
        imports.add("from typing import Annotated")
        annotations = ", ".join(check_annotations)
        return f"Annotated[{base_type}, {annotations}]", imports

    return base_type, imports


def generate_skeleton(spec: Spec, project_root: Path = Path(".")) -> None:
    """æœªå®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ"""
    print("ðŸ”¨ Generating skeleton code...")

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã«åŸºã¥ãå‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    app_root = project_root / "apps" / spec.meta.name
    print(f"  ðŸ“ Target directory: {app_root}")

    # Checké–¢æ•°ã®ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆï¼ˆåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¤‡æ•°ã®é–¢æ•°ã‚’è¿½åŠ ï¼‰
    check_files = {}  # file_path -> list of check functions
    for check in spec.checks:
        file_path = app_root / check.file_path
        if file_path not in check_files:
            check_files[file_path] = []
        check_files[file_path].append(check)

    for file_path, checks in check_files.items():
        if file_path.exists():
            print(f"  â­ï¸  Skip (exists): {file_path}")
            continue

        file_path.parent.mkdir(parents=True, exist_ok=True)

        # Generate all check functions for this file
        functions = []
        for check in checks:
            func_name = check.impl.split(":")[-1]
            func_code = f'''def {func_name}(payload: dict) -> bool:
    """{check.description}"""
    # TODO: implement validation logic
    return True
'''
            functions.append(func_code)

        code = f"""# Auto-generated skeleton for Check functions
{chr(10).join(functions)}
"""
        file_path.write_text(code)
        print(f"  âœ… Generated: {file_path}")

    # Transformé–¢æ•°ã®ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆ
    for transform in spec.transforms:
        file_path = app_root / transform.file_path
        if file_path.exists():
            print(f"  â­ï¸  Skip (exists): {file_path}")
            continue

        file_path.parent.mkdir(parents=True, exist_ok=True)
        func_name = transform.impl.split(":")[-1]

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        param_strs = []
        all_imports = set()

        for param in transform.parameters:
            type_str, imports = _build_type_annotation(spec, param, app_root)
            all_imports.update(imports)
            param_strs.append(f"{param.name}: {type_str}")

        # æˆ»ã‚Šå€¤ã®åž‹ã‚¢ãƒŽãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        return_type, return_imports = _build_return_annotation(
            spec, transform, app_root
        )
        all_imports.update(return_imports)

        # importæ–‡ã‚’ç”Ÿæˆï¼ˆspec2code.engineã‹ã‚‰ã®importã‚’çµ±åˆï¼‰
        import_lines = []
        spec2code_imports = set()
        other_imports = set()

        for imp in all_imports:
            if imp.startswith("from spec2code.engine import"):
                # spec2code.engineã‹ã‚‰ã®importã‚’æŠ½å‡º
                parts = imp.split("import", 1)[1].strip()
                spec2code_imports.add(parts)
            else:
                other_imports.add(imp)

        # spec2code.engineã®importã‚’çµ±åˆ
        if spec2code_imports:
            combined_import = (
                f"from spec2code.engine import {', '.join(sorted(spec2code_imports))}"
            )
            import_lines.append(combined_import)

        # ãã®ä»–ã®importã‚’è¿½åŠ 
        import_lines.extend(sorted(other_imports))

        import_section = "\n".join(import_lines) if import_lines else ""

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆä½œæˆ
        params = ", ".join(param_strs)

        code = f'''# Auto-generated skeleton for Transform: {transform.id}
{import_section}

def {func_name}({params}) -> {return_type}:
    """{transform.description}"""
    # TODO: implement transform logic
    return {{}}
'''
        file_path.write_text(code)
        print(f"  âœ… Generated: {file_path}")

    # __init__.py ã‚’å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä½œæˆ
    for directory in ["checks", "transforms", "datatypes"]:
        init_path = app_root / directory / "__init__.py"
        if not init_path.exists():
            init_path.parent.mkdir(parents=True, exist_ok=True)
            init_path.write_text("# Auto-generated\n")
            print(f"  âœ… Generated: {init_path}")


# ==================== DAGæ¤œè¨¼ãƒ»å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ ====================


class Engine:
    """ã‚³ã‚¢å®Ÿè¡Œãƒ»æ¤œè¨¼ãƒ»ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, spec: Spec):
        self.spec = spec
        self.graph = self._build_dag()

    def _build_dag(self) -> nx.DiGraph:
        """DAGã‚’æ§‹ç¯‰"""
        g = nx.DiGraph()

        # Transformã‚’ãƒŽãƒ¼ãƒ‰ã¨ã—ã¦è¿½åŠ 
        for transform in self.spec.transforms:
            g.add_node(transform.id)

        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for edge in self.spec.dag:
            if edge.to is None:
                # çµ‚ç«¯ãƒŽãƒ¼ãƒ‰
                continue
            g.add_edge(edge.from_, edge.to)

        # DAGæ¤œè¨¼ï¼ˆã‚µã‚¤ã‚¯ãƒ«ãŒãªã„ã‹ï¼‰
        if not nx.is_directed_acyclic_graph(g):
            raise ValueError("âŒ DAGã«ã‚µã‚¤ã‚¯ãƒ«ãŒå­˜åœ¨ã—ã¾ã™")

        return g

    def validate_schemas(self) -> None:
        """JSON Schemaæ¤œè¨¼"""
        print("ðŸ” Validating schemas...")
        for datatype in self.spec.datatypes:
            try:
                # ã‚¹ã‚­ãƒ¼ãƒžè‡ªä½“ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                jsonschema.Draft7Validator.check_schema(datatype.schema_def)
                print(f"  âœ… {datatype.id}: schema valid")
            except jsonschema.SchemaError as e:
                print(f"  âŒ {datatype.id}: schema invalid - {e}")

    def validate_integrity(
        self, project_root: Path = Path(".")
    ) -> dict[str, list[str]]:
        """ä»•æ§˜ã¨å®Ÿè£…ã®æ•´åˆæ€§ã‚’æ¤œè¨¼

        Returns:
            ã‚¨ãƒ©ãƒ¼ãƒžãƒƒãƒ— {category: [error_messages]}
        """
        print("ðŸ” Validating spec-implementation integrity...")
        errors: dict[str, list[str]] = {
            "check_functions": [],
            "check_locations": [],
            "transform_functions": [],
            "transform_signatures": [],
            "example_schemas": [],
        }

        # sys.pathã«packagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
        packages_dir = str((project_root / "packages").resolve())
        if packages_dir not in sys.path:
            sys.path.insert(0, packages_dir)

        app_root = project_root / "apps" / self.spec.meta.name

        # 1. Checké–¢æ•°ã®å­˜åœ¨ã¨ä½ç½®ã‚’æ¤œè¨¼
        for check in self.spec.checks:
            module_path, func_name = check.impl.split(":")
            expected_file = app_root / check.file_path

            # é–¢æ•°ãŒèª­ã¿è¾¼ã‚ã‚‹ã‹
            try:
                module = importlib.import_module(module_path)
                func = getattr(module, func_name)
                print(f"  âœ… Check {check.id}: function exists")

                # ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®ã®æ¤œè¨¼
                import inspect

                actual_file = Path(inspect.getfile(func)).resolve()
                expected_file_resolved = expected_file.resolve()

                if actual_file != expected_file_resolved:
                    error_msg = (
                        f"Check '{check.id}' location mismatch:\n"
                        f"    Expected: {expected_file}\n"
                        f"    Actual:   {actual_file}"
                    )
                    errors["check_locations"].append(error_msg)
                    print(f"  âš ï¸  {error_msg}")

            except (ImportError, AttributeError) as e:
                error_msg = f"Check '{check.id}' not found: {e}"
                errors["check_functions"].append(error_msg)
                print(f"  âŒ {error_msg}")

        # 2. Transformé–¢æ•°ã®å­˜åœ¨ã¨ä½ç½®ã‚’æ¤œè¨¼
        for transform in self.spec.transforms:
            module_path, func_name = transform.impl.split(":")
            expected_file = app_root / transform.file_path

            try:
                module = importlib.import_module(module_path)
                func = getattr(module, func_name)
                print(f"  âœ… Transform {transform.id}: function exists")

                # ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®ã®æ¤œè¨¼
                import inspect

                actual_file = Path(inspect.getfile(func)).resolve()
                expected_file_resolved = expected_file.resolve()

                if actual_file != expected_file_resolved:
                    error_msg = (
                        f"Transform '{transform.id}' location mismatch:\n"
                        f"    Expected: {expected_file}\n"
                        f"    Actual:   {actual_file}"
                    )
                    errors["transform_functions"].append(error_msg)
                    print(f"  âš ï¸  {error_msg}")

                # ã‚·ã‚°ãƒãƒãƒ£ã®æ¤œè¨¼
                sig = inspect.signature(func)
                expected_params = {p.name for p in transform.parameters}
                actual_params = set(sig.parameters.keys())

                if expected_params != actual_params:
                    error_msg = (
                        f"Transform '{transform.id}' signature mismatch:\n"
                        f"    Expected params: {sorted(expected_params)}\n"
                        f"    Actual params:   {sorted(actual_params)}"
                    )
                    errors["transform_signatures"].append(error_msg)
                    print(f"  âš ï¸  {error_msg}")

            except (ImportError, AttributeError) as e:
                error_msg = f"Transform '{transform.id}' not found: {e}"
                errors["transform_functions"].append(error_msg)
                print(f"  âŒ {error_msg}")

        # 3. Exampleå€¤ã®ã‚¹ã‚­ãƒ¼ãƒžé©åˆæ€§ã‚’æ¤œè¨¼
        for example in self.spec.examples:
            # ã“ã®ExampleãŒå‚ç…§ã•ã‚Œã¦ã„ã‚‹DataTypeã‚’æŽ¢ã™
            for datatype in self.spec.datatypes:
                if example.id in datatype.example_ids:
                    try:
                        jsonschema.validate(example.input, datatype.schema_def)
                        print(
                            f"  âœ… Example {example.id}: schema valid for {datatype.id}"
                        )
                    except jsonschema.ValidationError as e:
                        error_msg = (
                            f"Example '{example.id}' invalid for DataType '{datatype.id}':\n"
                            f"    {e.message}"
                        )
                        errors["example_schemas"].append(error_msg)
                        print(f"  âŒ {error_msg}")

        # ã‚µãƒžãƒªãƒ¼è¡¨ç¤º
        total_errors = sum(len(errs) for errs in errors.values())
        if total_errors == 0:
            print("\nâœ… All integrity checks passed!")
        else:
            print(f"\nâš ï¸  Found {total_errors} integrity issue(s)")

        return errors

    def run_checks(self) -> None:
        """Checké–¢æ•°ã‚’å®Ÿè¡Œ"""
        print("ðŸ” Running checks...")
        for check in self.spec.checks:
            module_path, func_name = check.impl.split(":")
            try:
                module = importlib.import_module(module_path)
                getattr(module, func_name)
                print(f"  âœ… {check.id}: loaded")
            except (ImportError, AttributeError) as e:
                print(f"  âŒ {check.id}: {e}")

    def run_examples(self) -> dict[str, bool]:
        """Exampleæ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        print("ðŸ§ª Running examples...")
        results = {}

        for example in self.spec.examples:
            # ç°¡æ˜“å®Ÿè£…: Transformã‚’å®Ÿè¡Œã—ã¦æœŸå¾…å€¤ã¨æ¯”è¼ƒ
            # å®Ÿéš›ã«ã¯DAGã‚’è¾¿ã£ã¦å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            print(f"  ðŸ”¬ {example.id}: {example.description}")
            results[example.id] = True  # TODO: å®Ÿéš›ã®æ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…

        return results

    def run_dag(self) -> None:
        """DAGã‚’å®Ÿè¡Œ"""
        print("ðŸš€ Running DAG...")

        if len(self.graph.nodes) == 0:
            print("  â„¹ï¸  No transforms to execute")
            return

        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆã§å®Ÿè¡Œé †åºã‚’æ±ºå®š
        try:
            execution_order = list(nx.topological_sort(self.graph))
        except nx.NetworkXError:
            print("  âŒ Cannot determine execution order (cycle detected)")
            return

        for transform_id in execution_order:
            # Transformå®šç¾©ã‚’å–å¾—
            transform = next(
                (t for t in self.spec.transforms if t.id == transform_id), None
            )
            if not transform:
                print(f"  âŒ {transform_id}: not found in spec")
                continue

            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»é–¢æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
            module_path, func_name = transform.impl.split(":")
            try:
                module = importlib.import_module(module_path)
                func = getattr(module, func_name)

                # ç°¡æ˜“å®Ÿè¡Œï¼ˆå¼•æ•°ã¯ä»®ï¼‰
                # å®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
                result = func(**transform.default_args)
                print(f"  âœ… {transform_id} -> {result}")
            except (ImportError, AttributeError) as e:
                print(f"  âŒ {transform_id}: {e}")
            except Exception as e:
                print(f"  âŒ {transform_id}: execution error - {e}")


# ==================== CLI ====================


def main():
    """CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse

    parser = argparse.ArgumentParser(description="Spec-to-Code Engine")
    subparsers = parser.add_subparsers(dest="command", help="ã‚µãƒ–ã‚³ãƒžãƒ³ãƒ‰")

    # gen ã‚³ãƒžãƒ³ãƒ‰
    gen_parser = subparsers.add_parser("gen", help="ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ")
    gen_parser.add_argument("spec_file", help="ä»•æ§˜ãƒ•ã‚¡ã‚¤ãƒ« (YAML/JSON)")

    # run ã‚³ãƒžãƒ³ãƒ‰
    run_parser = subparsers.add_parser("run", help="DAGå®Ÿè¡Œãƒ»æ¤œè¨¼")
    run_parser.add_argument("spec_file", help="ä»•æ§˜ãƒ•ã‚¡ã‚¤ãƒ« (YAML/JSON)")

    # validate ã‚³ãƒžãƒ³ãƒ‰
    validate_parser = subparsers.add_parser("validate", help="ä»•æ§˜ã¨å®Ÿè£…ã®æ•´åˆæ€§ã‚’æ¤œè¨¼")
    validate_parser.add_argument("spec_file", help="ä»•æ§˜ãƒ•ã‚¡ã‚¤ãƒ« (YAML/JSON)")

    # run-config ã‚³ãƒžãƒ³ãƒ‰
    run_config_parser = subparsers.add_parser(
        "run-config", help="Config-based DAG execution"
    )
    run_config_parser.add_argument("config_file", help="Config file (YAML)")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # run-config ã‚³ãƒžãƒ³ãƒ‰ã¯åˆ¥å‡¦ç†
    if args.command == "run-config":
        from packages.spec2code.config_runner import ConfigRunner
        import pandas as pd

        try:
            runner = ConfigRunner(args.config_file)

            # Create sample initial data (StepAFrame)
            initial_data = pd.DataFrame(
                {
                    "timestamp": [
                        "2024-01-01",
                        "2024-01-02",
                        "2024-01-03",
                        "2024-01-04",
                        "2024-01-05",
                    ],
                    "value": [100, 150, 120, 180, 140],
                }
            )

            print(f"\nðŸ“Š Initial data:")
            print(initial_data)
            print()

            result = runner.run(initial_data)

            print(f"\nðŸ“Š Final result:")
            print(result)

        except Exception as e:
            print(f"âŒ Config execution failed: {e}")
            import traceback

            traceback.print_exc()
            sys.exit(1)

        return

    # ä»•æ§˜èª­ã¿è¾¼ã¿
    try:
        spec = load_spec(args.spec_file)
        print(f"âœ… Loaded spec: {spec.meta.name} (v{spec.version})")
    except Exception as e:
        print(f"âŒ Failed to load spec: {e}")
        sys.exit(1)

    # ã‚³ãƒžãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == "gen":
        generate_skeleton(spec)
        print("âœ… Skeleton generation completed")

    elif args.command == "run":
        engine = Engine(spec)
        engine.validate_schemas()
        engine.run_checks()
        engine.run_dag()
        results = engine.run_examples()
        print(f"ðŸ“Š Example report: {results}")
        print("âœ… Execution completed")

    elif args.command == "validate":
        engine = Engine(spec)
        errors = engine.validate_integrity()

        # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã§çµ‚äº†
        total_errors = sum(len(errs) for errs in errors.values())
        if total_errors > 0:
            sys.exit(1)


if __name__ == "__main__":
    main()
