"""Validator: IRæ¤œè¨¼

IRã®æ„å‘³è«–ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã€‚
ä¸»ãªæ¤œè¨¼é …ç›®:
1. DataFrameå®šç¾©ã®å¦¥å½“æ€§ï¼ˆé‡è¤‡åˆ—ã€dtypeæœªè¨­å®šç­‰ï¼‰
2. Transformå®šç¾©ã®å¦¥å½“æ€§
3. DAG Stageå®šç¾©ã®å¦¥å½“æ€§
4. Pythonå‹å‚ç…§ã®è§£æ±ºå¯èƒ½æ€§
5. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ¤œè¨¼ï¼ˆå€™è£œã‚¼ãƒ­ã€checkã‚¼ãƒ­ã€exampleã‚¼ãƒ­ç­‰ï¼‰
"""

from __future__ import annotations

import importlib
from pathlib import Path

from spectool.spectool.core.base.ir import FrameSpec, SpecIR


def validate_ir(ir: SpecIR, skip_impl_check: bool = False) -> list[str]:
    """IRå…¨ä½“ã®æ„å‘³è«–ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR
        skip_impl_check: å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆgenæ™‚ã«ä½¿ç”¨ï¼‰

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆï¼ˆç©ºã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
    """
    errors: list[str] = []

    # DataFrameå®šç¾©ã®æ¤œè¨¼
    errors.extend(_validate_dataframe_specs(ir))

    # Checkå®šç¾©ã®æ¤œè¨¼
    errors.extend(_validate_check_specs(ir))

    # Transformå®šç¾©ã®æ¤œè¨¼
    errors.extend(_validate_transform_specs(ir))

    # DAG Stageå®šç¾©ã®æ¤œè¨¼
    errors.extend(_validate_dag_stage_specs(ir))

    # Pythonå‹å‚ç…§ã®æ¤œè¨¼ï¼ˆå®Ÿè£…ãƒã‚§ãƒƒã‚¯å«ã‚€ï¼‰
    errors.extend(_validate_type_references(ir, skip_impl_check=skip_impl_check))

    return errors


_VALID_DTYPES = {
    "int",
    "int8",
    "int16",
    "int32",
    "int64",
    "uint8",
    "uint16",
    "uint32",
    "uint64",
    "float",
    "float16",
    "float32",
    "float64",
    "str",
    "string",
    "bool",
    "boolean",
    "datetime",
    "datetime64",
    "datetime64[ns]",
    "timedelta",
    "timedelta64",
    "timedelta64[ns]",
    "object",
    "category",
}


def _validate_column_duplicates(frame: FrameSpec) -> list[str]:
    """é‡è¤‡åˆ—åã‚’ãƒã‚§ãƒƒã‚¯"""
    col_names = [col.name for col in frame.columns]
    duplicates = {name for name in col_names if col_names.count(name) > 1}
    if duplicates:
        return [f"DataFrame '{frame.id}': duplicate column names: {duplicates}"]
    return []


def _validate_column_dtypes(frame: FrameSpec) -> list[str]:
    """ã‚«ãƒ©ãƒ ã®dtypeã‚’ãƒã‚§ãƒƒã‚¯"""
    errors = []
    for col in frame.columns:
        if not col.dtype:
            errors.append(f"DataFrame '{frame.id}', column '{col.name}': dtype is not set")
        elif col.dtype.lower() not in _VALID_DTYPES:
            errors.append(
                f"DataFrame '{frame.id}', column '{col.name}': "
                f"invalid dtype '{col.dtype}'. Valid types: {_VALID_DTYPES}"
            )
    return errors


def _validate_index_dtype(frame: FrameSpec) -> list[str]:
    """Indexå®šç¾©ã®dtypeã‚’ãƒã‚§ãƒƒã‚¯"""
    errors = []
    if frame.index:
        if not frame.index.dtype:
            errors.append(f"DataFrame '{frame.id}': index dtype is not set")
        elif frame.index.dtype.lower() not in _VALID_DTYPES:
            errors.append(
                f"DataFrame '{frame.id}', index '{frame.index.name}': "
                f"invalid dtype '{frame.index.dtype}'. Valid types: {_VALID_DTYPES}"
            )
    return errors


def _validate_multiindex_dtypes(frame: FrameSpec) -> list[str]:
    """MultiIndexå®šç¾©ã®dtypeã‚’ãƒã‚§ãƒƒã‚¯"""
    errors = []
    if frame.multi_index:
        for level in frame.multi_index:
            if not level.dtype:
                errors.append(f"DataFrame '{frame.id}', MultiIndex level '{level.name}': dtype is not set")
            elif level.dtype.lower() not in _VALID_DTYPES:
                errors.append(
                    f"DataFrame '{frame.id}', MultiIndex level '{level.name}': "
                    f"invalid dtype '{level.dtype}'. Valid types: {_VALID_DTYPES}"
                )
    return errors


def _validate_dataframe_specs(ir: SpecIR) -> list[str]:
    """DataFrameå®šç¾©ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

    æ¤œè¨¼é …ç›®:
    - é‡è¤‡åˆ—å
    - dtypeæœªè¨­å®š
    - dtypeå€¤ã®å¦¥å½“æ€§
    - Index/MultiIndexã®å¦¥å½“æ€§

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    for frame in ir.frames:
        errors.extend(_validate_column_duplicates(frame))
        errors.extend(_validate_column_dtypes(frame))
        errors.extend(_validate_index_dtype(frame))
        errors.extend(_validate_multiindex_dtypes(frame))

    return errors


def _validate_check_specs(ir: SpecIR) -> list[str]:
    """Checkå®šç¾©ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

    æ¤œè¨¼é …ç›®:
    - implå‚ç…§ã®å½¢å¼ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    for check in ir.checks:
        # implå½¢å¼ãƒã‚§ãƒƒã‚¯
        if check.impl and check.impl.strip() != "" and ":" not in check.impl:
            errors.append(f"Check '{check.id}': impl must be in 'module:function' format, got '{check.impl}'")

    return errors


def _collect_all_datatype_ids(ir: SpecIR) -> set[str]:
    """å…¨datatype IDã‚’åé›†

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        å…¨datatype IDã®ã‚»ãƒƒãƒˆ
    """
    all_datatype_ids = set()
    all_datatype_ids.update(f.id for f in ir.frames)
    all_datatype_ids.update(e.id for e in ir.enums)
    all_datatype_ids.update(p.id for p in ir.pydantic_models)
    all_datatype_ids.update(t.id for t in ir.type_aliases)
    all_datatype_ids.update(g.id for g in ir.generics)
    return all_datatype_ids


def _validate_transform_specs(ir: SpecIR) -> list[str]:
    """Transformå®šç¾©ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

    æ¤œè¨¼é …ç›®:
    - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®type_refå¦¥å½“æ€§
    - return_type_refã®å¦¥å½“æ€§
    - implå‚ç…§ã®å½¢å¼ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    # å…¨datatype IDã‚’åé›†
    all_datatype_ids = _collect_all_datatype_ids(ir)

    for transform in ir.transforms:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®type_refæ¤œè¨¼
        for param in transform.parameters:
            if not param.type_ref:
                errors.append(f"Transform '{transform.id}', parameter '{param.name}': type_ref is not set")
                continue

            # nativeå‹ã¾ãŸã¯datatype_refã‹ãƒã‚§ãƒƒã‚¯
            if not _is_valid_type_ref(param.type_ref, all_datatype_ids):
                errors.append(
                    f"Transform '{transform.id}', parameter '{param.name}': invalid type_ref '{param.type_ref}'"
                )

        # return_type_refã®æ¤œè¨¼
        if transform.return_type_ref and not _is_valid_type_ref(transform.return_type_ref, all_datatype_ids):
            errors.append(f"Transform '{transform.id}': invalid return_type_ref '{transform.return_type_ref}'")

        # implå½¢å¼ãƒã‚§ãƒƒã‚¯
        if transform.impl and transform.impl.strip() != "" and ":" not in transform.impl:
            errors.append(
                f"Transform '{transform.id}': impl must be in 'module:function' format, got '{transform.impl}'"
            )

    return errors


def _validate_dag_stage_specs(ir: SpecIR) -> list[str]:
    """DAG Stageå®šç¾©ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

    æ¤œè¨¼é …ç›®:
    - candidatesã«å«ã¾ã‚Œã‚‹Transform IDã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    - default_transform_idã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    - selection_modeã®å¦¥å½“æ€§

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    # Transform IDã‚’åé›†
    transform_ids = {t.id for t in ir.transforms}

    for stage in ir.dag_stages:
        # candidatesã®æ¤œè¨¼
        for candidate_id in stage.candidates:
            if candidate_id not in transform_ids:
                errors.append(f"DAG Stage '{stage.stage_id}': candidate '{candidate_id}' not found in transforms")

        # default_transform_idã®æ¤œè¨¼
        if stage.default_transform_id and stage.default_transform_id not in stage.candidates:
            errors.append(
                f"DAG Stage '{stage.stage_id}': default_transform_id '{stage.default_transform_id}' not in candidates"
            )

        # selection_modeã®å¦¥å½“æ€§
        valid_modes = {"single", "exclusive", "multiple"}
        if stage.selection_mode not in valid_modes:
            errors.append(
                f"DAG Stage '{stage.stage_id}': invalid selection_mode '{stage.selection_mode}', "
                f"must be one of {valid_modes}"
            )

    return errors


def _validate_type_references(ir: SpecIR, skip_impl_check: bool = False) -> list[str]:
    """Pythonå‹å‚ç…§ã®è§£æ±ºå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯

    æ¤œè¨¼é …ç›®:
    - FrameSpec.row_modelã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½æ€§
    - FrameSpec.generator_factoryã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½æ€§
    - CheckSpec.implã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½æ€§
    - TransformSpec.implã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½æ€§

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR
        skip_impl_check: å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    # å®Ÿè£…ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
    if skip_impl_check:
        return errors

    # DataFrame row_modelã®æ¤œè¨¼
    for frame in ir.frames:
        if frame.row_model and not _can_import_python_ref(frame.row_model, ir):
            errors.append(f"DataFrame '{frame.id}': cannot import row_model '{frame.row_model}'")

        if frame.generator_factory and not _can_import_python_ref(frame.generator_factory, ir):
            errors.append(f"DataFrame '{frame.id}': cannot import generator_factory '{frame.generator_factory}'")

        # check_functionsã®æ¤œè¨¼
        for check_func in frame.check_functions:
            if not _can_import_python_ref(check_func, ir):
                errors.append(f"DataFrame '{frame.id}': cannot import check_function '{check_func}'")

    # Checké–¢æ•°ã®æ¤œè¨¼
    for check in ir.checks:
        if check.impl and not _can_import_python_ref(check.impl, ir):
            errors.append(f"Check '{check.id}': cannot import impl '{check.impl}'")

    # Transformé–¢æ•°ã®æ¤œè¨¼
    for transform in ir.transforms:
        if transform.impl and not _can_import_python_ref(transform.impl, ir):
            errors.append(f"Transform '{transform.id}': cannot import impl '{transform.impl}'")

    return errors


def _is_valid_type_ref(type_ref: str, all_datatype_ids: set[str]) -> bool:
    """å‹å‚ç…§ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

    Args:
        type_ref: å‹å‚ç…§æ–‡å­—åˆ—
        all_datatype_ids: å…¨datatype IDã®ã‚»ãƒƒãƒˆ

    Returns:
        å¦¥å½“ãªå‹å‚ç…§ã®å ´åˆTrue
    """
    # nativeå‹ã®å ´åˆï¼ˆ"builtins:int"å½¢å¼ï¼‰
    if type_ref.startswith("builtins:"):
        return True

    # datatype_refã®å ´åˆ
    if type_ref in all_datatype_ids:
        return True

    # Pythonå‹å‚ç…§ã®å ´åˆï¼ˆ"module:class"å½¢å¼ï¼‰
    return ":" in type_ref


def _resolve_impl_path(impl: str, ir: SpecIR) -> str:
    """implãƒ‘ã‚¹ã‚’è§£æ±ºï¼ˆapps. ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚€å½¢ã«å¤‰æ›ï¼‰

    Args:
        impl: å…ƒã®implãƒ‘ã‚¹ (ä¾‹: "apps.checks:func")
        ir: SpecIRï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåå–å¾—ç”¨ï¼‰

    Returns:
        è§£æ±ºã•ã‚ŒãŸimplãƒ‘ã‚¹ (ä¾‹: "apps.sample-project.checks:func")
    """
    if not impl.startswith("apps."):
        return impl

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—
    app_name = ir.meta.name if ir.meta else "app"

    # "apps." ã®å¾Œã®éƒ¨åˆ†ã‚’å–å¾—
    rest = impl[5:]  # "apps." ã‚’é™¤å»

    # "apps.<project-name>." + æ®‹ã‚Šã®éƒ¨åˆ†
    return f"apps.{app_name}.{rest}"


def _can_import_python_ref(ref: str, ir: SpecIR | None = None) -> bool:
    """Pythonå‹å‚ç…§ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯

    Args:
        ref: "module:class"å½¢å¼ã®å‚ç…§
        ir: SpecIRï¼ˆimplãƒ‘ã‚¹è§£æ±ºç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ãªå ´åˆTrue
    """
    if ":" not in ref:
        return False

    # implãƒ‘ã‚¹ã‚’è§£æ±º
    resolved_ref = ref
    if ir is not None:
        resolved_ref = _resolve_impl_path(ref, ir)

    try:
        module_path, name = resolved_ref.rsplit(":", 1)
        module = importlib.import_module(module_path)
        getattr(module, name)
        return True
    except (ImportError, AttributeError):
        return False


def _create_category_dict() -> dict[str, list[str]]:
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥è¾æ›¸ã‚’ä½œæˆ

    Returns:
        ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆè¾æ›¸
    """
    return {
        "dataframe_schemas": [],
        "datatypes": [],
        "check_definitions": [],
        "checks": [],
        "transform_definitions": [],
        "transforms": [],
        "dag_stages": [],
        "examples": [],
        "parameter_types": [],
        "edge_cases": [],
    }


def _categorize_error(error: str, errors: dict[str, list[str]]) -> None:
    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡

    Args:
        error: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        errors: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¨ãƒ©ãƒ¼è¾æ›¸
    """
    categorized = False

    # DataFrame schema errors
    if "DataFrame" in error and ("duplicate column" in error.lower() or "dtype is not set" in error):
        errors["dataframe_schemas"].append(error)
        categorized = True
    # DataFrame datatype errors
    elif "DataFrame" in error:
        errors["datatypes"].append(error)
        categorized = True

    # Check definition errors (impl related)
    if "Check" in error and "impl" in error:
        errors["check_definitions"].append(error)
        categorized = True
    # Other check errors
    elif "Check" in error and not categorized:
        errors["checks"].append(error)
        categorized = True

    # Transform definition errors (impl, type_ref, parameter related)
    if "Transform" in error and ("impl" in error or "type_ref" in error or "parameter" in error):
        errors["transform_definitions"].append(error)
        categorized = True
    # Other transform errors
    elif "Transform" in error and not categorized:
        errors["transforms"].append(error)
        categorized = True

    # DAG stage errors
    if ("DAG Stage" in error or "candidate" in error.lower()) and not categorized:
        errors["dag_stages"].append(error)
        categorized = True

    # Example errors
    if "Example" in error and not categorized:
        errors["examples"].append(error)
        categorized = True

    # Parameter type errors
    if "parameter" in error.lower() and ("type" in error.lower() or "default" in error.lower()) and not categorized:
        errors["parameter_types"].append(error)
        categorized = True

    # Edge cases (uncategorized)
    if not categorized:
        errors["edge_cases"].append(error)


def validate_spec(
    spec_path: str | Path,
    skip_impl_check: bool = False,
    normalize: bool = False,
) -> dict[str, dict[str, list[str]]]:
    """Spec YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ©ãƒ¼/è­¦å‘Š/æˆåŠŸã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¿”ã™

    Args:
        spec_path: Spec YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        skip_impl_check: å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆgenæ™‚ã«ä½¿ç”¨ï¼‰
        normalize: IRã‚’æ­£è¦åŒ–ã—ã¦ã‹ã‚‰æ¤œè¨¼ï¼ˆPydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰åˆ—ã‚’æ¨è«–ï¼‰

    Returns:
        3å±¤æ§‹é€ ã®è¾æ›¸: {"errors": {...}, "warnings": {...}, "successes": {...}}
        å„å±¤ã¯ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
    """
    import sys

    from spectool.spectool.core.engine.loader import load_spec

    spec_path = Path(spec_path)
    ir = load_spec(spec_path)

    # æ­£è¦åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if normalize:
        from spectool.spectool.core.engine.normalizer import normalize_ir

        ir = normalize_ir(ir)

    # sys.pathã«project_rootã‚’è¿½åŠ ï¼ˆapps.XXXå½¢å¼ã®importã®ãŸã‚ï¼‰
    # project_root ã¯ spec_path ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ä»®å®š
    project_root = spec_path.parent.resolve()
    project_root_str = str(project_root)
    if project_root_str not in sys.path:
        sys.path.insert(0, project_root_str)

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¾æ›¸ã‚’ä½œæˆ
    errors = _create_category_dict()
    warnings = _create_category_dict()
    successes = _create_category_dict()

    # æ—¢å­˜ã®validate_irã‚’å®Ÿè¡Œ
    flat_errors = validate_ir(ir, skip_impl_check=skip_impl_check)

    # ã‚¨ãƒ©ãƒ¼ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
    for error in flat_errors:
        _categorize_error(error, errors)

    # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ¤œè¨¼ã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ã®ã¿ï¼‰
    edge_case_errors = _validate_edge_cases_errors_only(ir)
    errors["edge_cases"].extend(edge_case_errors)

    # Exampleãƒ‡ãƒ¼ã‚¿ã®schemaæ¤œè¨¼
    example_data_errors = _validate_example_data(ir)
    errors["examples"].extend(example_data_errors)

    # è­¦å‘Šã‚’ç”Ÿæˆ
    datatype_check_warnings = _validate_datatype_checks(ir)
    warnings["datatypes"].extend(datatype_check_warnings)

    datatype_example_warnings = _validate_datatype_examples_generators(ir)
    warnings["datatypes"].extend(datatype_example_warnings)

    # æˆåŠŸã‚’è¨˜éŒ²
    _record_successes(ir, errors, successes)

    return {"errors": errors, "warnings": warnings, "successes": successes}


def _validate_edge_cases_errors_only(ir: SpecIR) -> list[str]:
    """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ¤œè¨¼ï¼ˆã‚¨ãƒ©ãƒ¼ã®ã¿ã€è­¦å‘Šã¯é™¤ãï¼‰

    æ¤œè¨¼é …ç›®:
    1. DAG stageã§å€™è£œtransformé–¢æ•°ãŒã‚¼ãƒ­ä»¶
    2. Exampleã®datatype_refå¦¥å½“æ€§
    3. Transformãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å‹ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    # å…¨datatype IDã‚’åé›†
    all_datatype_ids = _collect_all_datatype_ids(ir)

    # 1. DAG stageã®å€™è£œã‚¼ãƒ­ãƒã‚§ãƒƒã‚¯
    errors.extend(_validate_dag_stage_candidates(ir, all_datatype_ids))

    # 2. Exampleã®datatype_refå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    errors.extend(_validate_example_refs(ir, all_datatype_ids))

    # 3. Transformãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å‹ãƒã‚§ãƒƒã‚¯
    errors.extend(_validate_parameter_defaults(ir))

    return errors


def _find_matching_transforms(ir: SpecIR, input_type: str, output_type: str) -> list[str]:
    """æŒ‡å®šã•ã‚ŒãŸå…¥å‡ºåŠ›å‹ã«ä¸€è‡´ã™ã‚‹transformé–¢æ•°ã‚’æ¢ã™

    Args:
        ir: SpecIR
        input_type: å…¥åŠ›å‹
        output_type: å‡ºåŠ›å‹

    Returns:
        ä¸€è‡´ã™ã‚‹transform IDã®ãƒªã‚¹ãƒˆ
    """
    matching_transforms = []
    for transform in ir.transforms:
        # ç¬¬1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹ãŒinput_typeã§ã€return_type_refãŒoutput_typeã®ã‚‚ã®ã‚’æ¢ã™
        if transform.parameters and transform.return_type_ref == output_type:
            first_param = transform.parameters[0]
            if first_param.type_ref == input_type:
                matching_transforms.append(transform.id)
    return matching_transforms


def _validate_dag_stage_candidates(ir: SpecIR, all_datatype_ids: set[str]) -> list[str]:
    """DAG stageã®å€™è£œtransformé–¢æ•°ãŒã‚¼ãƒ­ä»¶ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR
        all_datatype_ids: å…¨datatype IDã®ã‚»ãƒƒãƒˆ

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    for stage in ir.dag_stages:
        # å€™è£œãŒç©ºã®å ´åˆ
        if not stage.candidates:
            # è‡ªå‹•åé›†ã‚’è©¦ã¿ã‚‹ï¼ˆinput_type â†’ output_type ã®å¤‰æ›ã‚’è¡Œã†transformã‚’æ¢ã™ï¼‰
            if stage.input_type and stage.output_type:
                matching_transforms = _find_matching_transforms(ir, stage.input_type, stage.output_type)

                if not matching_transforms:
                    errors.append(
                        f"DAG Stage '{stage.stage_id}': no transform candidates found for "
                        f"input_type '{stage.input_type}' â†’ output_type '{stage.output_type}'. "
                        f"Please define a transform or specify candidates explicitly."
                    )
            else:
                errors.append(
                    f"DAG Stage '{stage.stage_id}': candidates list is empty and "
                    f"input_type/output_type are not specified for auto-collection."
                )

    return errors


def _validate_datatype_checks(ir: SpecIR) -> list[str]:
    """DataTypeã®checké–¢æ•°ãŒã‚¼ãƒ­ä»¶ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šï¼‰

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    warnings: list[str] = []

    # FrameSpecã®check_functionsã‚’ãƒã‚§ãƒƒã‚¯
    for frame in ir.frames:
        if not frame.check_functions:
            warnings.append(
                f"DataType '{frame.id}': no check functions defined. "
                f"Consider adding check_functions for data validation."
            )

    # EnumSpecã®check_functionsã‚’ãƒã‚§ãƒƒã‚¯
    for enum in ir.enums:
        if not enum.check_functions:
            warnings.append(
                f"DataType '{enum.id}': no check functions defined. "
                f"Consider adding check_functions for data validation."
            )

    # PydanticModelSpecã®check_functionsã‚’ãƒã‚§ãƒƒã‚¯
    for model in ir.pydantic_models:
        if not model.check_functions:
            warnings.append(
                f"DataType '{model.id}': no check functions defined. "
                f"Consider adding check_functions for data validation."
            )

    # TypeAliasSpecã®check_functionsã‚’ãƒã‚§ãƒƒã‚¯
    for alias in ir.type_aliases:
        if not alias.check_functions:
            warnings.append(
                f"DataType '{alias.id}': no check functions defined. "
                f"Consider adding check_functions for data validation."
            )

    # GenericSpecã®check_functionsã‚’ãƒã‚§ãƒƒã‚¯
    for generic in ir.generics:
        if not generic.check_functions:
            warnings.append(
                f"DataType '{generic.id}': no check functions defined. "
                f"Consider adding check_functions for data validation."
            )

    return warnings


def _collect_example_datatypes(ir: SpecIR) -> set[str]:
    """Exampleã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹datatype_refã‚’åé›†"""
    example_datatypes = set()
    for example in ir.examples:
        if example.datatype_ref and example.datatype_ref.strip():
            example_datatypes.add(example.datatype_ref)
    return example_datatypes


def _collect_generator_datatypes(ir: SpecIR) -> set[str]:
    """Generatorã¨generator_factoryã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’åé›†"""
    generator_datatypes = set()
    for generator in ir.generators:
        if generator.return_type_ref and generator.return_type_ref.strip():
            generator_datatypes.add(generator.return_type_ref)

    # FrameSpecã®generator_factoryã‚‚ãƒã‚§ãƒƒã‚¯
    for frame in ir.frames:
        if frame.generator_factory:
            generator_datatypes.add(frame.id)

    return generator_datatypes


def _check_datatype_has_examples_or_generators(
    datatype_id: str, example_datatypes: set[str], generator_datatypes: set[str]
) -> str | None:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«exampleã¾ãŸã¯generatorãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    if datatype_id not in example_datatypes and datatype_id not in generator_datatypes:
        return (
            f"DataType '{datatype_id}': neither examples nor generators are defined. "
            f"Consider adding examples or a generator for testing."
        )
    return None


def _validate_datatype_examples_generators(ir: SpecIR) -> list[str]:
    """DataTypeã®example/generatorã®ä¸¡æ–¹ãŒã‚¼ãƒ­ä»¶ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šï¼‰

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    warnings: list[str] = []

    example_datatypes = _collect_example_datatypes(ir)
    generator_datatypes = _collect_generator_datatypes(ir)

    # å„DataTypeã§exampleã‚‚generatorã‚‚å­˜åœ¨ã—ãªã„ã‚‚ã®ã‚’è­¦å‘Š
    all_datatypes = [
        *ir.frames,
        *ir.enums,
        *ir.pydantic_models,
        *ir.type_aliases,
        *ir.generics,
    ]

    for datatype in all_datatypes:
        warning = _check_datatype_has_examples_or_generators(datatype.id, example_datatypes, generator_datatypes)
        if warning:
            warnings.append(warning)

    return warnings


def _validate_example_refs(ir: SpecIR, all_datatype_ids: set[str]) -> list[str]:
    """Exampleã®datatype_refå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR
        all_datatype_ids: å…¨datatype IDã®ã‚»ãƒƒãƒˆ

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    for example in ir.examples:
        # datatype_refãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆç©ºæ–‡å­—åˆ—ã§ãªã„ï¼‰
        if example.datatype_ref and example.datatype_ref.strip() and example.datatype_ref not in all_datatype_ids:
            errors.append(
                f"Example '{example.id}': datatype_ref '{example.datatype_ref}' not found in defined datatypes."
            )

    return errors


def _check_param_default_type(
    transform_id: str, param_name: str, default: object, type_ref: str, optional: bool
) -> str | None:
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®å‹ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        transform_id: Transform ID
        param_name: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å
        default: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        type_ref: å‹å‚ç…§
        optional: ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ•ãƒ©ã‚°

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ã¾ãŸã¯None
    """
    type_name = type_ref.split(":", 1)[1]
    expected_type = _get_builtin_type(type_name)

    if expected_type is None:
        return None

    # optional=Trueã®å ´åˆã¯Noneã‚’è¨±å®¹
    if optional and default is None:
        return None

    # intâ†’floatã®å¤‰æ›ã¯è¨±å®¹
    if expected_type is float and isinstance(default, int):
        return None

    # å‹ãŒä¸€è‡´ã—ãªã„å ´åˆ
    if not isinstance(default, expected_type):
        return (
            f"Transform '{transform_id}', parameter '{param_name}': "
            f"default value type mismatch. Expected {type_name}, "
            f"but got {type(default).__name__}."
        )

    return None


def _validate_parameter_defaults(ir: SpecIR) -> list[str]:
    """Transformãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å‹ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    for transform in ir.transforms:
        for param in transform.parameters:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
            if param.default is not None and param.type_ref.startswith("builtins:"):
                error = _check_param_default_type(
                    transform.id, param.name, param.default, param.type_ref, param.optional
                )
                if error:
                    errors.append(error)

    return errors


def _get_builtin_type(type_name: str) -> type | None:
    """builtinså‹åã‹ã‚‰å¯¾å¿œã™ã‚‹Pythonå‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—

    Args:
        type_name: å‹åï¼ˆ"int", "str", "float", "bool", "list", "dict"ç­‰ï¼‰

    Returns:
        å¯¾å¿œã™ã‚‹Pythonå‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ã¾ãŸã¯ None
    """
    builtin_types = {
        "int": int,
        "str": str,
        "float": float,
        "bool": bool,
        "list": list,
        "dict": dict,
        "tuple": tuple,
        "set": set,
    }
    return builtin_types.get(type_name)


def _validate_example_data(ir: SpecIR) -> list[str]:
    """Exampleã®inputãƒ‡ãƒ¼ã‚¿ãŒdatatype schemaã«é©åˆã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    errors: list[str] = []

    try:
        import pandas as pd
        import pandera as pa
    except ImportError:
        # pandera/pandasãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        return errors

    # FrameSpecã®ãƒãƒƒãƒ—ã‚’ä½œæˆ
    frame_map = {frame.id: frame for frame in ir.frames}

    for example in ir.examples:
        # datatype_refãŒFrameSpecã‚’å‚ç…§ã—ã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not example.datatype_ref or example.datatype_ref not in frame_map:
            continue

        frame = frame_map[example.datatype_ref]

        # inputãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not example.input:
            continue

        try:
            # inputã‚’DataFrameã«å¤‰æ›
            df = pd.DataFrame(example.input)

            # Panderaã‚¹ã‚­ãƒ¼ãƒã‚’å‹•çš„ã«ç”Ÿæˆã—ã¦æ¤œè¨¼
            columns = {}
            index_schema = None

            # Index
            if frame.index:
                index_dtype = _pandera_dtype_from_str(frame.index.dtype)
                if index_dtype:
                    index_schema = pa.Index(index_dtype)

            # Columns
            for col in frame.columns:
                col_dtype = _pandera_dtype_from_str(col.dtype)
                if col_dtype:
                    columns[col.name] = pa.Column(col_dtype, nullable=col.nullable, coerce=frame.coerce)

            # ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
            schema = pa.DataFrameSchema(columns, index=index_schema, coerce=frame.coerce)

            # æ¤œè¨¼å®Ÿè¡Œ
            schema.validate(df, lazy=True)

        except pa.errors.SchemaErrors:
            # è¤‡æ•°ã®ã‚¨ãƒ©ãƒ¼ã‚’ã¾ã¨ã‚ã¦å ±å‘Š
            error_msg = f"Example '{example.id}': input data violates schema for '{example.datatype_ref}'"
            errors.append(error_msg)
        except Exception as e:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ï¼ˆDataFrameã®ä½œæˆå¤±æ•—ãªã©ï¼‰
            errors.append(f"Example '{example.id}': failed to validate input data - {str(e)}")

    return errors


def _pandera_dtype_from_str(dtype_str: str) -> type | str | None:
    """dtypeæ–‡å­—åˆ—ã‚’Panderaå‹ã«å¤‰æ›

    Args:
        dtype_str: dtypeæ–‡å­—åˆ—

    Returns:
        Panderaå‹ã€ã¾ãŸã¯ None
    """
    mapping = {
        "int": int,
        "float": float,
        "str": str,
        "bool": bool,
        "datetime": "datetime64[ns]",
    }
    return mapping.get(dtype_str.lower())


def _record_successes(ir: SpecIR, errors: dict[str, list[str]], successes: dict[str, list[str]]) -> None:
    """æ¤œè¨¼ã«æˆåŠŸã—ãŸé …ç›®ã‚’è¨˜éŒ²ã™ã‚‹

    Args:
        ir: æ¤œè¨¼å¯¾è±¡ã®IR
        errors: ã‚¨ãƒ©ãƒ¼è¾æ›¸ï¼ˆã©ã®é …ç›®ã«ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹ã‹ç¢ºèªç”¨ï¼‰
        successes: æˆåŠŸè¾æ›¸ï¼ˆã“ã“ã«æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼‰
    """
    # ã™ã¹ã¦ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµåˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
    all_errors = " ".join([msg for msgs in errors.values() for msg in msgs])

    # DataFrame schemas ã®æˆåŠŸ
    for frame in ir.frames:
        if f"DataFrame '{frame.id}'" not in all_errors:
            successes["dataframe_schemas"].append(f"DataFrame '{frame.id}': schema is valid")

    # Check definitions ã®æˆåŠŸ
    for check in ir.checks:
        if f"Check '{check.id}'" not in all_errors:
            successes["check_definitions"].append(f"Check '{check.id}': definition is valid")

    # Transform definitions ã®æˆåŠŸ
    for transform in ir.transforms:
        if f"Transform '{transform.id}'" not in all_errors:
            successes["transform_definitions"].append(f"Transform '{transform.id}': definition is valid")

    # DAG stages ã®æˆåŠŸ
    for stage in ir.dag_stages:
        if f"DAG Stage '{stage.stage_id}'" not in all_errors:
            successes["dag_stages"].append(f"DAG Stage '{stage.stage_id}': configuration is valid")

    # Examples ã®æˆåŠŸ
    for example in ir.examples:
        if f"Example '{example.id}'" not in all_errors:
            successes["examples"].append(f"Example '{example.id}': datatype_ref is valid")


_CATEGORY_LABELS = {
    "dataframe_schemas": "ğŸ“Š DataFrame Schemas",
    "datatypes": "ğŸ”¤ Data Types",
    "check_definitions": "âœ“ Check Definitions",
    "checks": "âœ“ Checks",
    "transform_definitions": "ğŸ”„ Transform Definitions",
    "transforms": "ğŸ”„ Transforms",
    "dag_stages": "ğŸ“ˆ DAG Stages",
    "examples": "ğŸ“ Examples",
    "parameter_types": "âš™ï¸  Parameter Types",
    "edge_cases": "âš ï¸  Edge Cases",
}


def _format_message_category(category: str, messages: list[str], message_type: str) -> list[str]:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not messages:
        return []

    lines = []
    label = _CATEGORY_LABELS.get(category, category)
    count = len(messages)
    suffix = "s" if count > 1 else ""

    if message_type == "passed":
        lines.append(f"{label} ({count} {message_type}):")
    else:
        lines.append(f"{label} ({count} {message_type}{suffix}):")

    for msg in messages:
        lines.append(f"  â€¢ {msg}")
    lines.append("")
    return lines


def _format_errors(errors: dict[str, list[str]]) -> list[str]:
    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    total_errors = sum(len(msgs) for msgs in errors.values())
    if total_errors == 0:
        return []

    lines = [f"\nâŒ Validation failed with {total_errors} error(s):\n"]
    for category, messages in errors.items():
        lines.extend(_format_message_category(category, messages, "error"))
    return lines


def _format_warnings(warnings: dict[str, list[str]]) -> list[str]:
    """è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    total_warnings = sum(len(msgs) for msgs in warnings.values())
    if total_warnings == 0:
        return []

    lines = [f"\nâš ï¸  Found {total_warnings} warning(s):\n"]
    for category, messages in warnings.items():
        lines.extend(_format_message_category(category, messages, "warning"))
    return lines


def _format_successes(successes: dict[str, list[str]], verbose: bool) -> list[str]:
    """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆverboseãƒ¢ãƒ¼ãƒ‰ï¼‰"""
    if not verbose:
        return []

    total_successes = sum(len(msgs) for msgs in successes.values())
    if total_successes == 0:
        return []

    lines = [f"\nâœ… {total_successes} item(s) passed validation:\n"]
    for category, messages in successes.items():
        lines.extend(_format_message_category(category, messages, "passed"))
    return lines


def format_validation_result(result: dict[str, dict[str, list[str]]], verbose: bool = False) -> str:
    """æ¤œè¨¼çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦æ–‡å­—åˆ—ã«å¤‰æ›

    Args:
        result: validate_spec()ã®æˆ»ã‚Šå€¤
        verbose: è©³ç´°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆæˆåŠŸã‚‚è¡¨ç¤ºï¼‰

    Returns:
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ¤œè¨¼çµæœã®æ–‡å­—åˆ—
    """
    lines = []
    errors = result["errors"]
    warnings = result["warnings"]
    successes = result["successes"]

    # ã‚¨ãƒ©ãƒ¼ã€è­¦å‘Šã€æˆåŠŸã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    lines.extend(_format_errors(errors))
    lines.extend(_format_warnings(warnings))
    lines.extend(_format_successes(successes, verbose))

    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼ãŒãªã‘ã‚Œã°è¡¨ç¤ºï¼‰
    total_errors = sum(len(msgs) for msgs in errors.values())
    if total_errors == 0:
        lines.append("âœ… All validations passed")

    return "\n".join(lines)
