#!/usr/bin/env python
"""
Spectool CLI - Spec2Code Next Generation Command Line Interface

Usage:
    python -m spectool validate <spec_file>
    python -m spectool gen <spec_file> [--output-dir DIR]
    python -m spectool validate-integrity <spec_file>
    python -m spectool --version
"""

import sys
from pathlib import Path

import fire

from spectool.spectool.core.base.ir import SpecIR
from spectool.spectool.core.engine.loader import load_spec
from spectool.spectool.core.engine.normalizer import normalize_ir
from spectool.spectool.core.engine.validate import validate_spec, format_validation_result
from spectool.spectool.core.engine.integrity import IntegrityValidator
from spectool.spectool.core.engine.dag_runner import DAGRunner
from spectool.spectool.core.engine.config_runner import ConfigRunner
from spectool.spectool.backends.py_skeleton import generate_skeleton
from spectool.spectool.core.export.card_exporter import export_spec_to_cards

__version__ = "2.0.0-alpha"


class SpectoolCLI:
    """Spectool - Spec2Code Next Generation CLI"""

    def validate(self, spec_file: str, debug: bool = False, verbose: bool = False) -> None:
        """Validate spec file for correctness.

        Args:
            spec_file: Path to spec YAML file
            debug: Enable debug output
            verbose: Show detailed validation results including successes
        """
        spec_path = Path(spec_file)
        if not spec_path.exists():
            print(f"âŒ Error: Spec file not found: {spec_path}")
            sys.exit(1)

        try:
            # Load and validate spec with categorized results
            print(f"ðŸ“– Loading and validating spec: {spec_path}")
            result = validate_spec(str(spec_path), skip_impl_check=True, normalize=True)

            # Format and display results
            formatted = format_validation_result(result, verbose=verbose)
            print(formatted)

            # Exit with error if validation failed
            total_errors = sum(len(msgs) for msgs in result["errors"].values())
            if total_errors > 0:
                sys.exit(1)

        except Exception as e:
            print(f"âŒ Error: {e}")
            if debug:
                import traceback

                traceback.print_exc()
            sys.exit(1)

    def gen(self, spec_file: str, output_dir: str | None = None, debug: bool = False, verbose: bool = False) -> None:
        """Generate skeleton code from spec file.

        Args:
            spec_file: Path to spec YAML file
            output_dir: Output directory (default: current directory, generates apps/<project-name>/)
            debug: Enable debug output
            verbose: Show detailed validation results including successes
        """
        spec_path = Path(spec_file)
        if not spec_path.exists():
            print(f"âŒ Error: Spec file not found: {spec_path}")
            sys.exit(1)

        try:
            # Load and validate spec
            print(f"ðŸ“– Loading and validating spec: {spec_path}")
            ir = load_spec(str(spec_path))
            normalized = normalize_ir(ir)

            # Validate spec
            result = validate_spec(str(spec_path), skip_impl_check=True, normalize=True)

            # Format and display validation results
            formatted = format_validation_result(result, verbose=False)

            # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºã—ã¦çµ‚äº†
            total_errors = sum(len(msgs) for msgs in result["errors"].values())
            if total_errors > 0:
                print(formatted)
                sys.exit(1)

            # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºã—ã¦ç¶™ç¶š
            total_warnings = sum(len(msgs) for msgs in result["warnings"].values())
            if total_warnings > 0:
                print(formatted)
                print("Continuing with code generation...\n")
            else:
                print("âœ… Validation passed\n")

            # Determine output directory
            out_path = Path(output_dir) if output_dir else Path(".")

            print(f"ðŸ“ Output directory: {out_path}")

            # Generate complete skeleton using generate_skeleton
            print("ðŸ”¨ Generating skeleton code...")
            generate_skeleton(normalized, out_path)

            project_name = normalized.meta.name if normalized.meta else "generated-project"
            app_root = out_path / "apps" / project_name

            print("\nâœ… Skeleton generation complete!")
            print(f"   Generated project in: {app_root}")
            print("   Structure:")
            print("     - checks/     (validation functions)")
            print("     - transforms/ (processing functions)")
            print("     - generators/ (data generator functions)")
            print("     - models/     (Pydantic models)")
            print("     - schemas/    (Pandera validation schemas)")

        except Exception as e:
            print(f"âŒ Error: {e}")
            if debug:
                import traceback

                traceback.print_exc()
            sys.exit(1)

    def validate_integrity(self, spec_file: str, debug: bool = False) -> None:
        """Validate that implementation matches spec.

        Args:
            spec_file: Path to spec YAML file
            debug: Enable debug output
        """
        spec_path = Path(spec_file)
        if not spec_path.exists():
            print(f"âŒ Error: Spec file not found: {spec_path}")
            sys.exit(1)

        try:
            normalized = self._load_and_normalize_spec(spec_path)
            app_root = self._check_generated_directory(normalized)
            self._check_directory_structure(app_root)
            # Pass project root (current directory) instead of app_root
            self._run_integrity_validation(normalized, Path("."))
        except Exception as e:
            print(f"âŒ Error: {e}")
            if debug:
                import traceback

                traceback.print_exc()
            sys.exit(1)

    def _load_and_normalize_spec(self, spec_path: Path) -> SpecIR:
        """Load and normalize spec file."""
        print(f"ðŸ“– Loading spec: {spec_path}")
        ir = load_spec(str(spec_path))
        print(f"âœ… Loaded {len(ir.frames)} DataFrames, {len(ir.transforms)} Transforms")

        print("ðŸ”„ Normalizing IR...")
        normalized = normalize_ir(ir)
        print("âœ… Normalization complete")
        return normalized

    def _check_generated_directory(self, normalized: SpecIR) -> Path:
        """Check if generated directory exists."""
        project_name = normalized.meta.name if normalized.meta else "generated-project"
        app_root = Path("apps") / project_name

        print("ðŸ” Checking generated code...")
        print(f"  ðŸ“ Expected directory: {app_root}")

        if not app_root.exists():
            print("    âŒ Directory not found")
            print("\nâŒ Error: Generated code directory not found")
            print("   Run 'spectool gen' first to generate code.")
            sys.exit(1)
        print("    âœ… Directory exists")
        return app_root

    def _check_directory_structure(self, app_root: Path) -> None:
        """Check for required directories."""
        print("  ðŸ” Checking generated structure...")
        required_dirs = ["checks", "transforms", "generators", "models", "schemas"]

        for dirname in required_dirs:
            dir_path = app_root / dirname
            if dir_path.exists():
                print(f"    âœ… {dirname}/")
            else:
                print(f"    âš ï¸  {dirname}/ (missing, may not be needed)")

    def _run_integrity_validation(self, normalized: SpecIR, project_root: Path) -> None:
        """Run integrity validation.

        Args:
            normalized: Normalized SpecIR
            project_root: Project root directory (should be repository root, not apps/project_name)
        """
        print("  ðŸ” Validating implementation integrity...")
        validator = IntegrityValidator(normalized)
        result = validator.validate_integrity(project_root)

        total_errors = sum(len(errors) for errors in result.values())
        if total_errors > 0:
            print(f"\nâŒ Integrity validation failed with {total_errors} error(s)")
            for category, errors in result.items():
                if errors:
                    print(f"\n  {category}:")
                    for error in errors:
                        print(f"    âš ï¸  {error}")
            sys.exit(1)

        print("\nâœ… All integrity checks passed")
        print("   All required files exist and match spec")

    def run(
        self,
        spec_file: str,
        config: str | None = None,
        initial_data: str | None = None,
        debug: bool = False,
    ) -> None:
        """Execute DAG pipeline defined in spec.

        Args:
            spec_file: Path to spec YAML file (required)
            config: Path to config YAML file (optional, for parameter override)
            initial_data: Path to initial data file (JSON format, optional)
            debug: Enable debug output
        """
        spec_path = Path(spec_file)
        if not spec_path.exists():
            print(f"âŒ Error: Spec file not found: {spec_path}")
            sys.exit(1)

        try:
            if config:
                self._run_with_config(spec_path, config, initial_data)
            else:
                self._run_with_spec(spec_path, initial_data)
        except Exception as e:
            print(f"âŒ Error: {e}")
            if debug:
                import traceback

                traceback.print_exc()
            sys.exit(1)

    def _run_with_config(self, spec_path: Path, config: str, initial_data: str | None) -> None:
        """Execute DAG with config-driven approach."""
        config_path = Path(config)
        if not config_path.exists():
            print(f"âŒ Error: Config file not found: {config_path}")
            sys.exit(1)

        print(f"ðŸ“– Loading config: {config_path}")
        print(f"ðŸ“– Base spec: {spec_path}")

        runner = ConfigRunner(str(config_path))
        print(f"âœ… Loaded config: {runner.config.meta.config_name}")

        # Validate config
        print("ðŸ” Validating config...")
        validation_result = runner.validate(check_implementations=True)
        print("âœ… Config validation passed")

        # Show execution plan
        self._show_execution_plan(validation_result["execution_plan"])

        # Load and execute
        data = self._load_initial_data(initial_data)
        print("\nðŸš€ Executing DAG pipeline...")
        result = runner.run(data)
        print("\nâœ… Execution complete!")
        print(f"ðŸ“Š Result: {result}")

    def _run_with_spec(self, spec_path: Path, initial_data: str | None) -> None:
        """Execute DAG with spec-driven approach."""
        print(f"ðŸ“– Loading spec: {spec_path}")
        ir = load_spec(str(spec_path))
        print(f"âœ… Loaded {len(ir.transforms)} Transforms, {len(ir.dag_stages)} DAG stages")

        print("ðŸ”„ Normalizing IR...")
        normalized = normalize_ir(ir)
        print("âœ… Normalization complete")

        if not normalized.dag_stages:
            print("\nâŒ Error: No dag_stages defined in spec")
            print("   Either define dag_stages in spec or use --config to specify execution")
            sys.exit(1)

        # Build DAG runner
        print("ðŸ” Building DAG execution plan...")
        runner = DAGRunner(normalized)
        execution_order = runner.get_execution_order()
        print(f"âœ… Execution order: {[s.stage_id for s in execution_order]}")

        # Load and execute
        data = self._load_initial_data(initial_data)
        print("\nðŸš€ Executing DAG pipeline...")
        result = runner.run_dag(data)
        print("\nâœ… Execution complete!")
        print(f"ðŸ“Š Result: {result}")

    def _show_execution_plan(self, execution_plan: list) -> None:
        """Display execution plan."""
        print(f"\nðŸ“‹ Execution plan ({len(execution_plan)} step(s)):")
        for idx, step in enumerate(execution_plan, start=1):
            print(f"  {idx}. Stage: {step['stage_id']}")
            print(f"     Transform: {step['transform_id']}")
            if step["params"]:
                print(f"     Params: {step['params']}")

    def _load_initial_data(self, initial_data: str | None) -> dict:
        """Load initial data from file or return empty dict."""
        if initial_data:
            import json

            initial_data_path = Path(initial_data)
            if not initial_data_path.exists():
                print(f"âŒ Error: Initial data file not found: {initial_data_path}")
                sys.exit(1)

            with open(initial_data_path) as f:
                data = json.load(f)
            print(f"\nðŸ“Š Loaded initial data from: {initial_data_path}")
            return data

        print("\nâš ï¸  No initial data provided (use --initial-data to specify)")
        print("   Using empty dict as initial data")
        return {}

    def version(self) -> None:
        """Show version information."""
        print(f"spectool {__version__}")

    def _collect_referenced_card_keys(self, dag_stage_groups: list) -> set[str]:
        """DAGã‚¹ãƒ†ãƒ¼ã‚¸ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‚«ãƒ¼ãƒ‰ã‚­ãƒ¼ã‚’åŽé›†"""
        referenced_card_keys = set()

        for group in dag_stage_groups:
            related = group["related_cards"]

            # å˜ä¸€ã‚«ãƒ¼ãƒ‰
            for card_key in ["stage_card", "input_dtype_card", "output_dtype_card"]:
                if related.get(card_key):
                    card = related[card_key]
                    referenced_card_keys.add(f"{card['source_spec']}::{card['id']}")

            # ãƒªã‚¹ãƒˆåž‹ã‚«ãƒ¼ãƒ‰
            for card_list_key in [
                "transform_cards",
                "generator_cards",
                "param_dtype_cards",
                "input_example_cards",
                "output_example_cards",
                "input_check_cards",
                "output_check_cards",
            ]:
                for card in related.get(card_list_key, []):
                    referenced_card_keys.add(f"{card['source_spec']}::{card['id']}")

        return referenced_card_keys

    def _process_spec_file(self, spec_file: str) -> dict | None:
        """å˜ä¸€ã®specãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
        spec_path = Path(spec_file)
        if not spec_path.exists():
            print(f"âš ï¸  Skipped: {spec_file} (not found)")
            return None

        try:
            print(f"\nðŸ“– Processing: {spec_path}")
            ir = load_spec(str(spec_path))
            normalized = normalize_ir(ir)
            cards_data = export_spec_to_cards(normalized, spec_file)

            num_cards = len(cards_data["cards"])
            num_groups = len(cards_data["dag_stage_groups"])
            print(f"âœ… Exported {num_cards} cards, {num_groups} DAG stage groups from {spec_path.name}")

            return cards_data

        except Exception as e:
            print(f"âš ï¸  Skipped {spec_file}: {e}")
            return None

    def export_cards(self, *specs: str, output: str = "frontend/public/cards") -> None:
        """Export YAML specs to JSON cards for frontend display.

        Args:
            *specs: Spec YAML files (e.g., specs/*.yaml)
            output: Output directory (default: frontend/public/cards)

        Example:
            python -m spectool export-cards specs/*.yaml --output frontend/public/cards
        """
        import json

        if not specs:
            print("âŒ Error: No spec files provided")
            print("   Usage: spectool export-cards specs/*.yaml [--output DIR]")
            sys.exit(1)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path(output)
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ðŸ“ Output directory: {output_dir}")

        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        all_specs = []
        all_cards = []
        all_dag_stage_groups = []

        # å„specãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for spec_file in specs:
            cards_data = self._process_spec_file(spec_file)
            if cards_data:
                all_specs.append(cards_data["metadata"])
                all_cards.extend(cards_data["cards"])
                all_dag_stage_groups.extend(cards_data["dag_stage_groups"])

        # referenced_card_keysã¨unlinked_card_keysã‚’è¨ˆç®—
        all_card_keys = {f"{card['source_spec']}::{card['id']}" for card in all_cards}
        referenced_card_keys = self._collect_referenced_card_keys(all_dag_stage_groups)
        unlinked_card_keys = all_card_keys - referenced_card_keys

        # çµ±åˆJSONã‚’æ§‹ç¯‰
        output_data = {
            "specs": all_specs,
            "cards": all_cards,
            "dag_stage_groups": all_dag_stage_groups,
            "referenced_card_keys": sorted(list(referenced_card_keys)),
            "unlinked_card_keys": sorted(list(unlinked_card_keys)),
        }

        # all-cards.json ã«æ›¸ãè¾¼ã¿
        output_file = output_dir / "all-cards.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        print("\nâœ… Export complete!")
        print(f"   Total specs: {len(all_specs)}")
        print(f"   Total cards: {len(all_cards)}")
        print(f"   Total DAG stage groups: {len(all_dag_stage_groups)}")
        print(f"   Referenced cards: {len(referenced_card_keys)}")
        print(f"   Unlinked cards: {len(unlinked_card_keys)}")
        coverage_pct = 100 * len(referenced_card_keys) // len(all_cards)
        print(f"   Coverage: {len(referenced_card_keys)}/{len(all_cards)} ({coverage_pct}%)")
        print(f"   Output file: {output_file}")


def spectool_main() -> None:
    """Spectool CLI entry point (called from python -m spectool)."""
    fire.Fire(SpectoolCLI)


if __name__ == "__main__":
    spectool_main()
